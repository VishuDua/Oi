# Oi Voice Assistant ğŸ§ğŸ§ ğŸ—£ï¸
---

## ğŸŒŸ Features

### Backend (Voice Assistant Engine)

* ğŸ”Š **Speech-to-Text** with Whisper + Resemblyzer (Speaker Recognition)
* ğŸ§  **Text Generation** using Mistral-7B with Emotion Detection
* ğŸ—£ï¸ **Text-to-Speech** using Jenny TTS with audio playback
* ğŸ“‹ Transcript Logging and Conversation Memory
* ğŸ”„ RESTful APIs for all modules
* ğŸŒ¿ Emotion-adaptive tone (e.g., supportive if sadness is detected)
* â±ï¸ Multithreaded audio and model execution
* ğŸš€ Real-time voice activity detection

### Frontend (Chat UI)

* ğŸ–¥ï¸ **React-based Chat Interface**
* ğŸ’¬ Typing & Voice input toggling
* ğŸ™ï¸ Speech-to-text integration (planned)
* âš¡ Fast and responsive design
* ğŸ§ª Mock and real backend support

---

## ğŸ—ï¸ Folder Structure

```
Oi-Chatbot/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ Speech_to_text/        # Whisper + Resemblyzer
â”‚   â”‚   â”œâ”€â”€ api.py
â”‚   â”‚   â”œâ”€â”€ audio.py
â”‚   â”‚   â”œâ”€â”€ models.py
â”‚   â”‚   â”œâ”€â”€ speaker.py
â”‚   â”‚   â”œâ”€â”€ stt.py
â”‚   â”‚   â”œâ”€â”€ transcript.py
â”‚   â”‚   â””â”€â”€ utils.py
â”‚   â”œâ”€â”€ Text_GEN/              # Mistral + GoEmotions
â”‚   â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ modules/
â”‚   â”‚   â”œâ”€â”€ transcripts/
â”‚   â”‚   â””â”€â”€ text_gen.py
â”‚   â””â”€â”€ Text_To_Speech/        # Jenny TTS
â”‚       â””â”€â”€ tts.py
â”œâ”€â”€ Mistral/                   # Model files
â”œâ”€â”€ Transcripts/               # Transcripts
â”œâ”€â”€ Requirements.txt           # Main dependencies
â”œâ”€â”€ stt.py                     # STT entry point
â”œâ”€â”€ assistant_controller.py    # Process orchestrator
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ public/
â”‚   â””â”€â”€ src/
â”‚       â”œâ”€â”€ api/
â”‚       â”œâ”€â”€ components/
â”‚       â”œâ”€â”€ assets/
â”‚       â”œâ”€â”€ App.jsx
â”‚       â””â”€â”€ index.js
â””â”€â”€ README.md
```

---

## ğŸ”§ Installation & Setup

### 1ï¸âƒ£ Backend Setup

#### Prerequisites

* Python 3.8+
* (Recommended) CUDA-capable GPU
* Three Virtual Environments (main, text\_gen, tts)

#### Install Main Backend Dependencies

```bash
pip install -r Requirements.txt
```

#### Text Generation venv Setup

```bash
cd src/Text_GEN
python -m venv venv2
source venv2/bin/activate
pip install -r Requirements2.txt
```

#### Text-to-Speech venv Setup

```bash
cd src/Text_To_Speech
python -m venv venv
source venv/bin/activate
pip install TTS
```

#### Model Setup

* Place Mistral-7B GGUF model in `Mistral/`
* Ensure Whisper and Resemblyzer models are available
* Internet required to fetch GoEmotions labels

---

## ğŸ”  Frontend Setup

```bash
cd frontend
npm install
npm run dev
```

Frontend will start at [http://localhost:5173](http://localhost:5173)

Update `src/api/openai.js` with:

```js
export async function sendChatMessage(messages) {
  const res = await fetch("http://localhost:8989/respond", {
    method: "POST",
    headers: { "Content-Type": "application/json" },
    body: JSON.stringify({ input: messages[messages.length - 1].content })
  });
  const data = await res.json();
  return data.response;
}
```

---

## ğŸš€ Running the System

### Option A: Individual Modules

```bash
# TTS
python src/Text_To_Speech/tts.py
# STT
python stt.py
# Text Gen
python src/Text_GEN/text_gen.py
# Frontend
npm run dev
```

### Option B: Controller

```bash
python assistant_controller.py
```

Update the paths in `assistant_controller.py`:

```python
STT_ENV = "path_to_stt_venv/bin/python"
TEXT_GEN_ENV = "path_to_text_gen_venv/bin/python"
TTS_ENV = "path_to_tts_venv/bin/python"
```

---

## ğŸ“¡ API Endpoints

| Endpoint                       | Description                                    |
| ------------------------------ | ---------------------------------------------- |
| `POST /respond`                | Generate AI response from text or spoken input |
| `POST /tts?text=Hello`         | Convert text to speech (Jenny TTS)             |
| `POST /chat?text=Hello`        | Respond and speak                              |
| `GET /transcript?mode=plain`   | Raw transcript log                             |
| `GET /transcript?mode=speaker` | Speaker-attributed transcript                  |
| `/toggle_mode`                 | Toggle between voice/text mode                 |

---

## ğŸ§° Technologies Used

* **Backend**: Python, FastAPI, Flask, Torch, Transformers
* **Speech**: Faster Whisper, Resemblyzer, SoundDevice
* **LLM**: llama-cpp, Mistral 7B Instruct
* **Emotion**: GoEmotions (BERT-based classifier)
* **Frontend**: React.js, Vite, TailwindCSS

---

## ğŸšª Usage Examples

```bash
# Basic TTS
curl -X POST 'http://localhost:6969/tts?text=Hello%20world!'

# Chat with TTS
curl -X POST 'http://localhost:6969/chat?text=How%20are%20you?'
```

---

## ğŸ”§ Troubleshooting

* **Audio Issues**: Check microphone, sounddevice, audio config
* **Model Loading**: Ensure correct model paths, enough RAM/GPU
* **API Errors**: Verify ports and endpoints match

---

## ğŸ’ª Contributing

* Fork, clone, and PR your improvements
* Open GitHub issues for bugs and features
* Improve frontend integration or backend performance

---

## ğŸ“„ License

MIT License

---

## Credits

* Mistral AI
* Coqui TTS (Jenny Model)
* Hugging Face
* Faster Whisper
* Resemblyzer
* Google GoEmotions
